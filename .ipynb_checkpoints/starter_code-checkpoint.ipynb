{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import tifffile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)\n",
    "is_cuda = device.type == 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train-clean-tif`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset: 52\n",
      "Number of images in the validation dataset: 16\n"
     ]
    }
   ],
   "source": [
    "# Loading TIFF images\n",
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.filenames = [f for f in os.listdir(directory) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.directory, self.filenames[idx])\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform: # Dynamically apply data transformation\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Create a transform to convert the images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0,translate=(0.1,0.1)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset for training images\n",
    "train_dataset = TIFFDataset('train-clean-tif', transform=train_transform)\n",
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(train_dataset, batch_size):\n",
    "    torch.manual_seed(0)  # For reproducibility of random numbers in PyTorch\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Creates a training DataLoader from this Dataset\n",
    "    return train_loader\n",
    "\n",
    "dataset_size = len(train_dataset), len(val_dataset)\n",
    "print('Number of images in the training dataset:', dataset_size[0])\n",
    "print('Number of images in the validation dataset:', dataset_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a denoising network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only one convolutional layer and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrivialNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrivialNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        initial_weights = torch.tensor([\n",
    "            [[1e-6, 1e-6, 1e-6],\n",
    "            [1e-6, 1, 1e-6],\n",
    "            [1e-6, 1e-6, 1e-6]]\n",
    "        ], dtype=torch.float32) #initial weights for the skip layers, 1e-6 is close to 0\n",
    "        \n",
    "        # Encoder Section\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "        \n",
    "        #Performing convolution and batch normalization on first skip layer\n",
    "        self.conv_skip1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, padding=1)\n",
    "        self.conv_skip_bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_skip2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, padding=1)\n",
    "        self.conv_skip_bn2 = nn.BatchNorm2d(256)\n",
    "        self.conv_skip3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, padding=1)\n",
    "        self.conv_skip_bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv_skip4 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, padding=1)\n",
    "        self.conv_skip_bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv_skip5 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, padding=1)\n",
    "        self.conv_skip_bn5 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output 128x128\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output 64x64\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output 32x32\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 4\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "        self.conv8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output 16x16\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        # layer 5\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1) # output 16x16\n",
    "        self.conv10 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1) # output 16x16\n",
    "\n",
    "        # Decoder Section\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2) # output 32x32\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 4\n",
    "        self.conv11 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1) # output 32x32; still using 1024 channels because of concatenation\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2) # output 64x64\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.dropout6 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1) # output 64x64; still using 512 channels because of concatenation\n",
    "        self.conv14 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2) # output 128x128\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.dropout7 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv15 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1) # output 128x128; still using 256 channels because of concatenation\n",
    "        self.conv16 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2) # output 256x256\n",
    "        self.bn8 = nn.BatchNorm2d(64)\n",
    "        self.dropout8 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 1\n",
    "        self.conv17 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1) # output 256x256; still using 128 channels because of concatenation\n",
    "        self.conv18 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "\n",
    "        # Output layer\n",
    "        self.conv19 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0) # output 256x256\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x11 = self.relu(self.conv1(x))\n",
    "        x12 = self.relu(self.conv2(x11))\n",
    "\n",
    "        x_skip1 = self.relu(self.conv_skip1(x12))\n",
    "        x_skip1 = self.conv_skip_bn1(x_skip1)\n",
    "        x_skip2 = self.relu(self.conv_skip2(x_skip1))\n",
    "        x_skip2 = self.conv_skip_bn2(x_skip2)\n",
    "        x_skip3 = self.relu(self.conv_skip3(x_skip2))\n",
    "        x_skip3 = self.conv_skip_bn3(x_skip3)\n",
    "        x_skip4 = self.relu(self.conv_skip4(x_skip3))\n",
    "        x_skip4 = self.conv_skip_bn4(x_skip4)\n",
    "        x_skip5 = self.relu(self.conv_skip5(x_skip4))\n",
    "        x_skip5 = self.conv_skip_bn5(x_skip5)\n",
    "\n",
    "        \n",
    "        x2 = self.bn1(self.pool1(x_skip5))\n",
    "        x21 = self.relu(self.conv3(x2))\n",
    "        x22 = self.relu(self.conv4(x21))\n",
    "\n",
    "        x3 = self.bn2(self.pool2(x22))\n",
    "        x31 = self.relu(self.conv5(x3))\n",
    "        x32 = self.relu(self.conv6(x31))\n",
    "\n",
    "        x4 = self.bn3(self.pool3(x32))\n",
    "        x41 = self.relu(self.conv7(x4))\n",
    "        x42 = self.relu(self.conv8(x41))\n",
    "\n",
    "        x5 = self.bn4(self.pool4(x42))\n",
    "        x51 = self.relu(self.conv9(x5))\n",
    "        x52 = self.relu(self.conv10(x51))\n",
    "\n",
    "        x6 = self.bn5(self.up1(x52))\n",
    "        x6 = torch.cat((x6, x42), dim=1)\n",
    "        x61 = self.relu(self.conv11(x6))\n",
    "        x62 = self.relu(self.conv12(x61))\n",
    "\n",
    "        x7 = self.bn6(self.up2(x62))\n",
    "        x7 = torch.cat((x7, x32), dim=1)\n",
    "        x71 = self.relu(self.conv13(x7))\n",
    "        x72 = self.relu(self.conv14(x71))\n",
    "\n",
    "        x8 = self.bn7(self.up3(x72))\n",
    "        x8 = torch.cat((x8, x22), dim=1)\n",
    "        x81 = self.relu(self.conv15(x8))\n",
    "        x82 = self.relu(self.conv16(x81))\n",
    "\n",
    "        x9 = self.bn8(self.up4(x82))\n",
    "        x9 = torch.cat((x9, x_skip5), dim=1)\n",
    "        x91 = self.relu(self.conv17(x9))\n",
    "        x92 = self.relu(self.conv18(x91))\n",
    "\n",
    "        x = self.conv19(x92)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, train_loader, test_loader, num_epoch, noise_std, avg_train_losses=[], avg_test_losses=[], epoch=0):\n",
    "\n",
    "    for epoch in range(epoch, num_epoch): # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i, y_tr_batch in enumerate(train_loader): # Loop over mini-batches\n",
    "            if is_cuda:\n",
    "                y_tr_batch = y_tr_batch.to(device) #GPU STUFF\n",
    "            x_tr_batch = y_tr_batch.clone()\n",
    "            # implement data augmentation\n",
    "            # flip = torch.randint(0,1,(1,)).item()\n",
    "            # if flip:\n",
    "            #     x_tr_batch = x_tr_batch.flip(2) # flip the image horizontally\n",
    "            # rotate = torch.randint(-10, 10,(1,)).item()\n",
    "            # translate = torch.randint(-10, 10, (2,)).tolist()\n",
    "            # x_tr_batch = TF.affine(x_tr_batch, angle=rotate, translate=(translate), scale=1, shear=0)\n",
    "            noise = torch.randn_like(y_tr_batch) * noise_std\n",
    "            x_tr_batch = x_tr_batch + noise\n",
    "\n",
    "            # insert translation, rotation, and flipping\n",
    "            opt.zero_grad() # delete previous gradients\n",
    "            y_hat_tr_batch = model(x_tr_batch) # forward pass\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch) # compute loss\n",
    "            loss.backward() # backward pass\n",
    "            opt.step() # update weights\n",
    "            total_train_loss += loss.item() # accumulate loss\n",
    "            # if (i + 1) % 10 == 0:\n",
    "                # print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader) # compute average loss\n",
    "        avg_train_losses.append(avg_train_loss) # accumulate average loss\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0\n",
    "            for i, y_te_batch in enumerate(test_loader):\n",
    "                if is_cuda:\n",
    "                    y_te_batch= y_te_batch.to(device)#GPU STUFF\n",
    "                noise = torch.randn_like(y_te_batch) * noise_std\n",
    "                x_te_batch = y_te_batch + noise\n",
    "                y_hat_te_batch = model(x_te_batch)\n",
    "                loss = criterion(y_hat_te_batch, y_te_batch)\n",
    "                total_test_loss += loss.item()\n",
    "            avg_test_loss = total_test_loss / len(test_loader)\n",
    "            print(f'Epoch {epoch+1}, Test Loss: {avg_test_loss:.6f}, Train Loss: {avg_train_loss:.6f}')\n",
    "            avg_test_losses.append(avg_test_loss)\n",
    "\n",
    "        # Save the model and optimizer state\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'avg_train_losses': avg_train_losses[0:epoch],\n",
    "                'avg_test_losses': avg_test_losses[0:epoch]\n",
    "            }, 'model_checkpoint.pt')\n",
    "\n",
    "    print('Length of Training loss', len(avg_test_losses))\n",
    "    print('Length of Testing loss:', len(avg_test_losses))\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(range(1, num_epoch+1), avg_train_losses, label='training loss')\n",
    "    ax.plot(range(1, num_epoch+1), avg_test_losses, label='testing loss')\n",
    "    ax.plot()\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('NMSE loss')\n",
    "    # ax.set_yscale('log')  # Set the vertical axis to log scale\n",
    "    ax.set_title('training loss')\n",
    "    ax.legend(['training accuracy', 'test accuracy'])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2  # Number of complete images in each batch\n",
    "lr = 1e-3  # Learning rate\n",
    "sig = 0.1  # Noise std\n",
    "num_epoch = 200  # Epochs\n",
    "\n",
    "# Create a test loader (using validation for testing)\n",
    "test_loader = create_loader(val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initilize the model, criterion, and optimizer\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "train_loader = create_loader(train_dataset, batch_size)\n",
    "model = TrivialNet() # Pick a model\n",
    "if is_cuda:\n",
    "    model = model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=lr) # Pick an optimizer\n",
    "criterion = nn.MSELoss() # Pick a loss function\n",
    "# criterion = nmse_loss\n",
    "avg_train_losses = []\n",
    "avg_test_losses = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if there is a checkpoint file, if there is, it will load the model and optimizer state\n",
    "\n",
    "# load in checkpoint if it exists and train\n",
    "if os.path.exists('model_checkpoint.pt'):\n",
    "    checkpoint = torch.load('model_checkpoint.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    avg_train_losses = checkpoint['avg_train_losses']\n",
    "    avg_test_losses = checkpoint['avg_test_losses']\n",
    "    epoch = checkpoint['epoch']\n",
    "    print('Model loaded from checkpoint at epoch', checkpoint['epoch'])\n",
    "    print('Length of Training loss', len(avg_test_losses))\n",
    "    print('Length of Testing loss:', len(avg_test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 32 but got size 33 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model (from scratch or a checkpoint)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_model(model, opt, criterion, train_loader, test_loader, num_epoch, sig, avg_train_losses, avg_test_losses, epoch)\n",
      "Cell \u001b[1;32mIn[34], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, opt, criterion, train_loader, test_loader, num_epoch, noise_std, avg_train_losses, avg_test_losses, epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# insert translation, rotation, and flipping\u001b[39;00m\n\u001b[0;32m     21\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# delete previous gradients\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m y_hat_tr_batch \u001b[38;5;241m=\u001b[39m model(x_tr_batch) \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat_tr_batch, y_tr_batch) \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# backward pass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Spring_24\\ML\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Spring_24\\ML\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 131\u001b[0m, in \u001b[0;36mTrivialNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    128\u001b[0m x52 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv10(x51))\n\u001b[0;32m    130\u001b[0m x6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn5(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x52))\n\u001b[1;32m--> 131\u001b[0m x6 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x6, x42), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    132\u001b[0m x61 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv11(x6))\n\u001b[0;32m    133\u001b[0m x62 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv12(x61))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 32 but got size 33 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# train model (from scratch or a checkpoint)\n",
    "train_model(model, opt, criterion, train_loader, test_loader, num_epoch, sig, avg_train_losses, avg_test_losses, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it to one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "val_clean = val_dataset[0] # Load one clean image from the validation dataset\n",
    "val_noisy = val_clean + (torch.randn_like(val_clean) * sig) # Add noise to the clean image\n",
    "# val_denoised = model(val_noisy).detach() # Denoise the noisy image using the trained model\n",
    "# val_clean_fft = torch.fft.fft2(val_clean)\n",
    "val_noisy_4d = val_noisy.unsqueeze(0) # Add an extra dimension to represent the batch size\n",
    "val_denoised = model(val_noisy_4d).detach() # Now pass this 4D tensor to the model\n",
    "\n",
    "# Remove the batch dimension before further processing\n",
    "val_denoised = val_denoised.squeeze(0)\n",
    "val_clean_FFT = np.fft.fftn(val_clean)\n",
    "val_noisy_fft = torch.fft.fftn(val_noisy)\n",
    "val_denoised_fft = torch.fft.fftn(val_denoised)\n",
    "\n",
    "\n",
    "# Your existing code to generate the figure and axes\n",
    "fig, ax = plt.subplots(3, 3, figsize=(10, 7))\n",
    "\n",
    "# Plot clean image\n",
    "ax[0, 0].imshow(np.abs(val_clean).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('clean image')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "# Plot noisy image\n",
    "ax[0, 1].imshow(np.abs(val_noisy).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('noisy image')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "# Plot denoised image\n",
    "ax[0, 2].imshow(np.abs(val_denoised).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('denoised image')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "# Plot corresponding error images\n",
    "ax[1, 0].imshow(3*np.abs(val_clean - val_clean).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 0].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(3*np.abs(val_clean - val_noisy).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 1].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(3*np.abs(val_clean - val_denoised).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 2].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "# Plot the Fourier transform of the clean image\n",
    "ax[2, 0].imshow(np.log(np.abs(np.fft.fftshift(val_clean_FFT.squeeze()))**2), cmap='gray')\n",
    "ax[2, 0].axis('off')\n",
    "\n",
    "# Plot the Fourier transform of the noisy image\n",
    "ax[2, 1].imshow(np.log(np.abs(np.fft.fftshift(val_noisy_fft.squeeze()))**2), cmap='gray')\n",
    "ax[2, 1].axis('off')\n",
    "\n",
    "# Plot the Fourier transform of the denoised image\n",
    "ax[2, 2].imshow(np.log(np.abs(np.fft.fftshift(val_denoised_fft.squeeze()))**2), cmap='gray')\n",
    "ax[2, 2].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMSE loss\n",
    "def nmse_loss(output, target):\n",
    "    diff = output - target\n",
    "    diff_norm_squared = torch.sum(diff ** 2) ** (1/2)\n",
    "    target_norm_squared = torch.sum(target ** 2) ** (1/2)\n",
    "    nmse = diff_norm_squared / (target_norm_squared + 1e-8)  # Add a small constant to avoid division by zero\n",
    "    return 20 * torch.log10(nmse)  # Convert to dB\n",
    "\n",
    "# load in validation dataset\n",
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "\n",
    "# Create a data loader for the validation dataset\n",
    "val_loader = create_loader(val_dataset, batch_size)\n",
    "\n",
    "# Evaluate performance on the validation set using NMSE\n",
    "criterion = nmse_loss\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_nmse = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        noisy_batch = val_batch + (torch.randn_like(val_batch) * sig)\n",
    "        denoised_batch = model(noisy_batch)\n",
    "        nmse = criterion(denoised_batch, val_batch)\n",
    "        total_nmse += nmse.item()\n",
    "nmse = total_nmse / len(val_loader)\n",
    "\n",
    "print('Validation Normalized Mean Squared Error:', nmse)\n",
    "print(min(100-5*(nmse+20.7), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
