{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0779cb53-a916-41f9-ab95-c24039d89a16",
   "metadata": {},
   "source": [
    "___\n",
    "# **BME 5710 project report**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca22128-d705-4915-9165-1138474e2eaf",
   "metadata": {},
   "source": [
    "___\n",
    "### Provide descriptive answers at `???` locations and insert figures or tables at `?content?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d1a16-3368-4f9f-94d1-771dba37e494",
   "metadata": {},
   "source": [
    "___\n",
    "### Write your name below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14effb-5721-472d-b299-c311b1edb084",
   "metadata": {},
   "source": [
    "Answer: Armon Sekhavat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a46acc",
   "metadata": {},
   "source": [
    "___\n",
    "### Write the name of your teammates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756447a",
   "metadata": {},
   "source": [
    "Answer: Garrett Herb & Arian Seighali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af677ae3-4e2b-49df-a06a-771af1154d91",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.1) Provide a layout of your CNN (6%)\n",
    "\n",
    "#### The layout should provide all the necessary details about the CNN architecture including number of channels, size of convolution kernels, activation functions, etc. For inspiration, see examples [here](https://www.geeksforgeeks.org/u-net-architecture-explained/), [here](https://www.researchgate.net/figure/The-architecture-of-Unet_fig2_334287825), [here](https://www.researchgate.net/figure/Modified-U-net-network-architecture_fig2_356216368), and [here](https://open-instruction.com/dl-algorithms/overview-of-residual-neural-network-resnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738302d6-ccf3-49af-85c7-eac2de4ed5e2",
   "metadata": {},
   "source": [
    "Image is attached in submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876c54",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.2) List *all* non-trivial features of your CNN and the training process. This may include use of dropout, learning rate scheduling, transfer learning, data augmentation, etc. Don't include items that are already covered in the layout provided above.  (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f65401",
   "metadata": {},
   "source": [
    "Data augmantation: \n",
    "Translation of 0.1 in both x and y direction\n",
    "Flip with a probability of 0.5 \n",
    "\n",
    "Batch normmalization:\n",
    "BN implemented to all upsampling and downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc63e9",
   "metadata": {},
   "source": [
    "___\n",
    "### (2) Answer the following questions about your CNN architecture and training. (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819917d8-0b7d-4cf9-9636-60764389eb90",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.1) Provide at least ten hyperparameters that you *could* optimize in your CNN design and training. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82087b67-e000-489a-95ff-0015550911a5",
   "metadata": {},
   "source": [
    "1) Learning rate\n",
    "2) Dropout rate (for all 8 convloution layers)\n",
    "3) Flip rate\n",
    "4) Number of Epochs\n",
    "5) Batch Size\n",
    "6) Weight of SSIM in loss function\n",
    "7) Weight of L1 in loss function\n",
    "9) Beta 1 for Adam\n",
    "10) Beta 2 for Adam\n",
    "11) Weight decay for Adam\n",
    "\n",
    "Bounds for translational bounds, Bounds for rotation***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e191e-148d-4da1-89e7-f44e40cff7f0",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.2) Now, list the hyperparameters that you *have* optimized. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f0d19-fca5-45cc-a9d9-d9cb87dde1cf",
   "metadata": {},
   "source": [
    "1) Number of epochs \n",
    "2) Batch size \n",
    "3) Weight of SSIM and L1 in loss function\n",
    "4) Droupout Rate\n",
    "\n",
    "rotational bounds, translational bounds**** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa6b3-c2e7-48af-bf3a-88d912c015ab",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.3) Describe your stretegy/approach for optimizing hyperparameters. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5850bf-b2b9-4541-8c10-cc5cd9a4651a",
   "metadata": {},
   "source": [
    "Answer: We started with the orginal U-Net architecture, and form there each team member took turns turns implementing some change to the model, that being a tweak to a particular hyperparameter, or an adjustment to the model design. Tried a high number of epochs, 500, then saw a plateu. Initially increased batch size (6), got worse, then decreased it, and it got better. Founded how much SSIM and L1 norm was contributing to loss function, then added weights to help balance this contribution (didn't actually change weights since didn't make a difference. Tried different dropout settings (.1 and .2), never made it better (worse in some cases), dropout at first and last layer did help reduce difference between test and training loss, but test loss did not improve. Adding rotation made it worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d7bc2-ed30-432f-bf37-556ca4bdd575",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.4) What loss function and optimizers did you use?  Express the loss function mathematically. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29d8db-783d-45f6-aef1-d63b16c29570",
   "metadata": {},
   "source": [
    "$$ L1 = \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$n$ is the total number of samples\n",
    "\n",
    "\n",
    "$\\hat{y}_i$ is the predicted value of the i-th sample\n",
    "\n",
    "$| \\cdot |$ denotes the absolute value\n",
    "\n",
    "$$\n",
    "\\text{SSIM} = \\frac{{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}}{{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}}\n",
    "$$\n",
    "\n",
    "$\\mu_x$ and $\\mu_y$: These represent the mean values of the pixel intensities in the original image $(\\mu_x)$ and the distorted image $(\\mu_y)$, respectively. The mean value is calculated by summing up all the pixel intensities and dividing by the total number of pixels.\n",
    "\n",
    "$C_1$ and $C_2$: These are constants added to the formula to stabilize the division when the denominator approaches zero. They are typically small positive values.\n",
    "\n",
    "$\\sigma_{xy}$: This represents the covariance between the pixel intensities of the original and distorted images. It measures the linear relationship between the two images.\n",
    "\n",
    "$\\sigma_x$ and $\\sigma_y$: These represent the standard deviations of the pixel intensities in the original image $(\\sigma_x)$ and the distorted image $(\\sigma_y)$, respectively. The standard deviation is a measure of the spread or variability of the pixel intensities.\n",
    "\n",
    "*Adam was used as the optimizer for this model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12df09-b84d-47d7-a0a8-52356975c8d3",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.5) Calculate the number of learnable parameters in your final CNN. How does that number compare with the number of training samples? Is your network overfitting or underfitting and how did you arrive at that conclusion? Explain that in the context of loss vs. epoch and NMSE vs. epoch curves for training and validation datasets. (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "024088be-2e9b-428c-8341-d8a90a747e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of Learnable Parameters:  31034433\n"
     ]
    }
   ],
   "source": [
    "# Considering batch norm has 2 learnable parameters per channel:\n",
    "layer1 = (3 * 3 * 1 * 64 + 64) + (3 * 3 * 64 * 64 + 64)\n",
    "layer2 = (2 * 64) + (3 * 3 * 64 * 128 + 128) + (3 * 3 * 128 * 128 + 128)\n",
    "layer3 = (2 * 128) + (3 * 3 * 128 * 256 + 256) + (3 * 3 * 256 * 256 + 256)\n",
    "layer4 = (2 * 256) + (3 * 3 * 256 * 512 + 512) + (3 * 3 * 512 * 512 + 512)\n",
    "layer5 = (2 * 512) + (3 * 3 * 512 * 1024 + 1024) + (3 * 3 * 1024 * 1024 + 1024)\n",
    "layer6 = (2 * 2 * 1024 * 512 + 512) + (2 * 512) + (3 * 3 * 1024 * 512 + 512) + (3 * 3 * 512 * 512 + 512)\n",
    "layer7 = (2 * 2 * 512 * 256 + 256) + (2 * 256) + (3 * 3 * 512 * 256 + 256) + (3 * 3 * 256 * 256 + 256)\n",
    "layer8 = (2 * 2 * 256 * 128 + 128) + (2 * 128) + (3 * 3 * 256 * 128 + 128) + (3 * 3 * 128 * 128 + 128)\n",
    "layer9 = (2 * 2 * 128 * 64 + 64) + (2 * 64) + (3 * 3 * 128 * 64 + 64) + (3 * 3 * 64 * 64 + 64) + (1 * 1 * 64 * 1 + 1)\n",
    "total = layer1 + layer2 + layer3 + layer4 + layer5 + layer6 + layer7 + layer8 + layer9\n",
    "print(\"Total # of Learnable Parameters: \", total)\n",
    "\n",
    "#Overfitting, training samples always perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb81cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "### (3.1) Insert (or draw using Markdown) a table that summarizes NMSE and SSIM for (i) noisy images, (ii) images denoised with TrivialNet model included in `starter_code.ipynb`, and (iii) images denoised with your CNN. Include the metrics from training, validation, and test datasets. (6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436b3fa-15e3-44ef-9b59-9c7f687fb80b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Per our last run:\n",
    "\n",
    "|             | Noisy vs Clean | TrivialNet vs Clean | DeNet vs Clean |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Training NMSE      | -7.935601       | -0.580696       | -19.698793       |\n",
    "| Training SSIM   | 0.639227        | -0.049679       | 0.962014       |\n",
    "| Validation NMSE       | -8.009263       | -0.609903       | -19.472027       |\n",
    "| Validation SSIM       | 0.647057       | -0.057882       | 0.962750       |\n",
    "| Testing NMSE       | -7.956191       | -0.522039       | -19.660973       |\n",
    "| Testing SSIM       | 0.674077       | -0.059171       | 0.967844       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308882b",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.2) When it comes to comparing images, what does SSIM capture that NMSE does not? (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814291f",
   "metadata": {},
   "source": [
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83ccbb",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.1) Display a figure where the first row (from left to right) shows an example of clean image, noisy image, image denoised with TrivialNet, and the image denoised with your CNN, and the second row shows corresponding error maps after 3-fold amplification. Select the image from the test dataset. (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7daed",
   "metadata": {},
   "source": [
    "Image is attached in submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c89ab4-45de-4bd2-ac00-57eed8ed6e95",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.2) From (4.1), identify which image features are well-preserved by your denoiser and which are lost. Additionally, describe how you could further improve the performance of your denoiser if given more time and resources. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3b816",
   "metadata": {},
   "source": [
    "Adding addtional data, addtional convolutional layers, fine tuning learning rate/trying something other than adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
