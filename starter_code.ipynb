{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# **Starter code for BME 5710 project**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import tifffile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('gpu' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train-clean-tif`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset: 52\n",
      "Number of images in the validation dataset: 16\n"
     ]
    }
   ],
   "source": [
    "# Loading TIFF images\n",
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.filenames = [f for f in os.listdir(directory) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.directory, self.filenames[idx])\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform: # Dynamically apply data transformation\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Create a transform to convert the images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0,translate=(0.1,0.1)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset for training images\n",
    "train_dataset = TIFFDataset('train-clean-tif', transform=train_transform)\n",
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(train_dataset, batch_size):\n",
    "    torch.manual_seed(0)  # For reproducibility of random numbers in PyTorch\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Creates a training DataLoader from this Dataset\n",
    "    return train_loader\n",
    "\n",
    "dataset_size = len(train_dataset), len(val_dataset)\n",
    "print('Number of images in the training dataset:', dataset_size[0])\n",
    "print('Number of images in the validation dataset:', dataset_size[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a denoising network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only one convolutional layer and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrivialNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Encoder Section\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output 128x128\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output 64x64\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output 32x32\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 4\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "        self.conv8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output 16x16\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        # layer 5\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1) # output 16x16\n",
    "        self.conv10 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1) # output 16x16\n",
    "\n",
    "        # Decoder Section\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2) # output 32x32\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 4\n",
    "        self.conv11 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1) # output 32x32; still using 1024 channels because of concatenation\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output 32x32\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2) # output 64x64\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.dropout6 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1) # output 64x64; still using 512 channels because of concatenation\n",
    "        self.conv14 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output 64x64\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2) # output 128x128\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.dropout7 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv15 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1) # output 128x128; still using 256 channels because of concatenation\n",
    "        self.conv16 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # output 128x128\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2) # output 256x256\n",
    "        self.bn8 = nn.BatchNorm2d(64)\n",
    "        self.dropout8 = nn.Dropout(0.2)\n",
    "\n",
    "        # layer 1\n",
    "        self.conv17 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1) # output 256x256; still using 128 channels because of concatenation\n",
    "        self.conv18 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # output 256x256\n",
    "\n",
    "        # Output layer\n",
    "        self.conv19 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0) # output 256x256\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x11 = self.relu(self.conv1(x))\n",
    "        x12 = self.relu(self.conv2(x11))\n",
    "\n",
    "        x2 = self.bn1(self.pool1(x12))\n",
    "        x21 = self.relu(self.conv3(x2))\n",
    "        x22 = self.relu(self.conv4(x21))\n",
    "\n",
    "        x3 = self.bn2(self.pool2(x22))\n",
    "        x31 = self.relu(self.conv5(x3))\n",
    "        x32 = self.relu(self.conv6(x31))\n",
    "\n",
    "        x4 = self.bn3(self.pool3(x32))\n",
    "        x41 = self.relu(self.conv7(x4))\n",
    "        x42 = self.relu(self.conv8(x41))\n",
    "\n",
    "        x5 = self.bn4(self.pool4(x42))\n",
    "        x51 = self.relu(self.conv9(x5))\n",
    "        x52 = self.relu(self.conv10(x51))\n",
    "\n",
    "        x6 = self.bn5(self.up1(x52))\n",
    "        x6 = torch.cat((x6, x42), dim=1)\n",
    "        x61 = self.relu(self.conv11(x6))\n",
    "        x62 = self.relu(self.conv12(x61))\n",
    "\n",
    "        x7 = self.bn6(self.up2(x62))\n",
    "        x7 = torch.cat((x7, x32), dim=1)\n",
    "        x71 = self.relu(self.conv13(x7))\n",
    "        x72 = self.relu(self.conv14(x71))\n",
    "\n",
    "        x8 = self.bn7(self.up3(x72))\n",
    "        x8 = torch.cat((x8, x22), dim=1)\n",
    "        x81 = self.relu(self.conv15(x8))\n",
    "        x82 = self.relu(self.conv16(x81))\n",
    "\n",
    "        x9 = self.bn8(self.up4(x82))\n",
    "        x9 = torch.cat((x9, x12), dim=1)\n",
    "        x91 = self.relu(self.conv17(x9))\n",
    "        x92 = self.relu(self.conv18(x91))\n",
    "\n",
    "        x = self.conv19(x92)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, train_loader, test_loader, num_epoch, noise_std, avg_train_losses=[], avg_test_losses=[], epoch=0):\n",
    "\n",
    "    for epoch in range(epoch, num_epoch): # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i, y_tr_batch in enumerate(train_loader): # Loop over mini-batches\n",
    "            x_tr_batch = y_tr_batch.clone()\n",
    "            # implement data augmentation\n",
    "            # flip = torch.randint(0,1,(1,)).item()\n",
    "            # if flip:\n",
    "            #     x_tr_batch = x_tr_batch.flip(2) # flip the image horizontally\n",
    "            # rotate = torch.randint(-10, 10,(1,)).item()\n",
    "            # translate = torch.randint(-10, 10, (2,)).tolist()\n",
    "            # x_tr_batch = TF.affine(x_tr_batch, angle=rotate, translate=(translate), scale=1, shear=0)\n",
    "            noise = torch.randn_like(y_tr_batch) * noise_std\n",
    "            x_tr_batch = x_tr_batch + noise\n",
    "\n",
    "            # insert translation, rotation, and flipping\n",
    "            opt.zero_grad() # delete previous gradients\n",
    "            y_hat_tr_batch = model(x_tr_batch) # forward pass\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch) # compute loss\n",
    "            loss.backward() # backward pass\n",
    "            opt.step() # update weights\n",
    "            total_train_loss += loss.item() # accumulate loss\n",
    "            # if (i + 1) % 10 == 0:\n",
    "                # print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader) # compute average loss\n",
    "        avg_train_losses.append(avg_train_loss) # accumulate average loss\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0\n",
    "            for i, y_te_batch in enumerate(test_loader):\n",
    "                noise = torch.randn_like(y_te_batch) * noise_std\n",
    "                x_te_batch = y_te_batch + noise\n",
    "                y_hat_te_batch = model(x_te_batch)\n",
    "                loss = criterion(y_hat_te_batch, y_te_batch)\n",
    "                total_test_loss += loss.item()\n",
    "            avg_test_loss = total_test_loss / len(test_loader)\n",
    "            print(f'Epoch {epoch+1}, Test Loss: {avg_test_loss:.6f}, Train Loss: {avg_train_loss:.6f}')\n",
    "            avg_test_losses.append(avg_test_loss)\n",
    "\n",
    "        # Save the model and optimizer state\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'avg_train_losses': avg_train_losses[0:epoch],\n",
    "                'avg_test_losses': avg_test_losses[0:epoch]\n",
    "            }, 'model_checkpoint.pt')\n",
    "\n",
    "    print('Length of Training loss', len(avg_test_losses))\n",
    "    print('Length of Testing loss:', len(avg_test_losses))\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(range(1, num_epoch+1), avg_train_losses, label='training loss')\n",
    "    ax.plot(range(1, num_epoch+1), avg_test_losses, label='testing loss')\n",
    "    ax.plot()\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('NMSE loss')\n",
    "    # ax.set_yscale('log')  # Set the vertical axis to log scale\n",
    "    ax.set_title('training loss')\n",
    "    ax.legend(['training accuracy', 'test accuracy'])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # Number of complete images in each batch\n",
    "lr = 1e-3  # Learning rate\n",
    "sig = 0.1  # Noise std\n",
    "num_epoch = 100  # Epochs\n",
    "\n",
    "# Create a test loader (using validation for testing)\n",
    "test_loader = create_loader(val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize the model, criterion, and optimizer\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "train_loader = create_loader(train_dataset, batch_size)\n",
    "model = TrivialNet() # Pick a model\n",
    "opt = optim.Adam(model.parameters(), lr=lr) # Pick an optimizer\n",
    "criterion = nn.MSELoss() # Pick a loss function\n",
    "# criterion = nmse_loss\n",
    "avg_train_losses = []\n",
    "avg_test_losses = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint at epoch 99\n",
      "Length of Training loss 99\n",
      "Length of Testing loss: 99\n"
     ]
    }
   ],
   "source": [
    "# Checks to see if there is a checkpoint file, if there is, it will load the model and optimizer state\n",
    "\n",
    "# load in checkpoint if it exists and train\n",
    "if os.path.exists('model_checkpoint.pt'):\n",
    "    checkpoint = torch.load('model_checkpoint.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    avg_train_losses = checkpoint['avg_train_losses']\n",
    "    avg_test_losses = checkpoint['avg_test_losses']\n",
    "    epoch = checkpoint['epoch']\n",
    "    print('Model loaded from checkpoint at epoch', checkpoint['epoch'])\n",
    "    print('Length of Training loss', len(avg_test_losses))\n",
    "    print('Length of Testing loss:', len(avg_test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Loss: 0.003306, Train Loss: 0.012037\n",
      "Epoch 2, Test Loss: 0.002010, Train Loss: 0.001908\n",
      "Epoch 3, Test Loss: 0.001389, Train Loss: 0.001524\n",
      "Epoch 4, Test Loss: 0.001198, Train Loss: 0.001288\n",
      "Epoch 5, Test Loss: 0.001152, Train Loss: 0.001187\n",
      "Epoch 6, Test Loss: 0.001117, Train Loss: 0.001152\n",
      "Epoch 7, Test Loss: 0.001174, Train Loss: 0.001121\n",
      "Epoch 8, Test Loss: 0.001079, Train Loss: 0.001095\n",
      "Epoch 9, Test Loss: 0.001106, Train Loss: 0.001061\n"
     ]
    }
   ],
   "source": [
    "# train model (from scratch or a checkpoint)\n",
    "train_model(model, opt, criterion, train_loader, test_loader, num_epoch, sig, avg_train_losses, avg_test_losses, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it to one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the clean image: torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m val_clean \u001b[38;5;241m=\u001b[39m val_dataset[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Load one clean image from the validation dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of the clean image:\u001b[39m\u001b[38;5;124m'\u001b[39m, val_clean\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m val_noisy \u001b[38;5;241m=\u001b[39m val_clean \u001b[38;5;241m+\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn_like(val_clean) \u001b[38;5;241m*\u001b[39m sig) \u001b[38;5;66;03m# Add noise to the clean image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m val_denoised \u001b[38;5;241m=\u001b[39m model(val_noisy)\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m# Denoise the noisy image using the trained model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# val_clean_fft = torch.fft.fft2(val_clean)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sig' is not defined"
     ]
    }
   ],
   "source": [
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "val_clean = val_dataset[0] # Load one clean image from the validation dataset\n",
    "val_noisy = val_clean + (torch.randn_like(val_clean) * sig) # Add noise to the clean image\n",
    "val_denoised = model(val_noisy).detach() # Denoise the noisy image using the trained model\n",
    "# val_clean_fft = torch.fft.fft2(val_clean)\n",
    "val_clean_FFT = np.fft.fftn(val_clean)\n",
    "val_noisy_fft = torch.fft.fftn(val_noisy)\n",
    "val_denoised_fft = torch.fft.fftn(val_denoised)\n",
    "\n",
    "\n",
    "# Your existing code to generate the figure and axes\n",
    "fig, ax = plt.subplots(3, 3, figsize=(10, 7))\n",
    "\n",
    "# Plot clean image\n",
    "ax[0, 0].imshow(np.abs(val_clean).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('clean image')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "# Plot noisy image\n",
    "ax[0, 1].imshow(np.abs(val_noisy).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('noisy image')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "# Plot denoised image\n",
    "ax[0, 2].imshow(np.abs(val_denoised).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('denoised image')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "# Plot corresponding error images\n",
    "ax[1, 0].imshow(3*np.abs(val_clean - val_clean).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 0].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(3*np.abs(val_clean - val_noisy).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 1].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(3*np.abs(val_clean - val_denoised).squeeze().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, r'$\\times 3$', transform=ax[1, 2].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "# Plot the Fourier transform of the clean image\n",
    "ax[2, 0].imshow(np.log(np.abs(np.fft.fftshift(val_clean_FFT.squeeze()))**2), cmap='gray')\n",
    "ax[2, 0].axis('off')\n",
    "\n",
    "# Plot the Fourier transform of the noisy image\n",
    "ax[2, 1].imshow(np.log(np.abs(np.fft.fftshift(val_noisy_fft.squeeze()))**2), cmap='gray')\n",
    "ax[2, 1].axis('off')\n",
    "\n",
    "# Plot the Fourier transform of the denoised image\n",
    "ax[2, 2].imshow(np.log(np.abs(np.fft.fftshift(val_denoised_fft.squeeze()))**2), cmap='gray')\n",
    "ax[2, 2].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Normalized Mean Squared Error: -15.782816529273987\n",
      "75.41408264636993\n"
     ]
    }
   ],
   "source": [
    "# define NMSE loss\n",
    "def nmse_loss(output, target):\n",
    "    diff = output - target\n",
    "    diff_norm_squared = torch.sum(diff ** 2) ** (1/2)\n",
    "    target_norm_squared = torch.sum(target ** 2) ** (1/2)\n",
    "    nmse = diff_norm_squared / (target_norm_squared + 1e-8)  # Add a small constant to avoid division by zero\n",
    "    return 20 * torch.log10(nmse)  # Convert to dB\n",
    "\n",
    "# load in validation dataset\n",
    "val_dataset = TIFFDataset('val-clean-tif', transform=transform) # Create the dataset for validation images\n",
    "\n",
    "# Create a data loader for the validation dataset\n",
    "val_loader = create_loader(val_dataset, batch_size)\n",
    "\n",
    "# Evaluate performance on the validation set using NMSE\n",
    "criterion = nmse_loss\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_nmse = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        noisy_batch = val_batch + (torch.randn_like(val_batch) * sig)\n",
    "        denoised_batch = model(noisy_batch)\n",
    "        nmse = criterion(denoised_batch, val_batch)\n",
    "        total_nmse += nmse.item()\n",
    "nmse = total_nmse / len(val_loader)\n",
    "\n",
    "print('Validation Normalized Mean Squared Error:', nmse)\n",
    "print(min(89-5*(nmse + 18.5), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
